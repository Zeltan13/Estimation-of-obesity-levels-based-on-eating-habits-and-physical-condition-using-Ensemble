# -*- coding: utf-8 -*-
"""Tubes CLO4 Estimation of obesity levels based on eating habits and physical condition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13KFZ6BRh1_n_qZSHLpItsfjleFRLPZ4n

# <center>Project-Based Assignment CLO-4</center>

<b>Topik</b>
Implementation of Ensemble Method on Classification Task

<b>Anggota Kelompok</b>
1. ABDUL AZIS AL AYUBBI - 1301213493
2. FATHAN ASKAR - 1301213374
3. DHAFINDRA RAZAQA STEFANO - 1301213115

####Latar Belakang

Latar belakang laporan ini dibuat adalah untuk memenuhi tugas besar mata kuliah Pembelajaran Mesin. Pada laporan ini, kami diharuskan menjelaskan, mendesain, dan mengimplementasikan ensemble method yang telah dipelajari selama kuliah. Dengan didasari oleh dataset yang telah kami pilih, yaitu "Estimation of Obesity Levels Based On Eating Habits and Physical Condition". Dataset tersebut berisikan data untuk mengestimasi tingkat obesitas dalam individual dari 3 negara yaitu Meksiko, Peru, dan Kolombia. Dataset ini dibuat secara sintetis menggunakan Weka tool dan SMOTE filter, namun 23% dari data tersebut didapatkan langsung dari pengguna melalui platform web.

Terdapat beberapa ensemble method yang tersedia, untuk laporan ini kami hanya akan membahas 3 diantaranya, yaitu Bagging dengan Random Forest, Boosting dengan Gradient Boosting, dan juga Stacking. Pada laporan ini, kami mencoba ketiga method tersebut, untuk mengetahui metode mana yang akan digunakan untuk model baseline, yang pada akhirnya akan digunakan untuk membandingkan model-model klasifikasi lainnya yang menggunakan ensemble method yang sama.

#### Exploration Data Analysis
"""

import pandas as pd
import  numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive

drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/ObesityDataSet_raw_and_data_sinthetic_noChange.csv'
data = pd.read_csv(file_path)

data.head(10)

data.info()
#Tidak ada data null
#data type harus di ubah soalnya beda-beda

print("Unique values in the 'family_history_with_overweight' column:", data['family_history_with_overweight'].unique())
print("Unique values in the 'FAVC' column:", data['FAVC'].unique())
print("Unique values in the 'CAEC' column:", data['CAEC'].unique())
print("Unique values in the 'SMOKE' column:", data['SMOKE'].unique())
print("Unique values in the 'SCC' column:", data['SCC'].unique())
print("Unique values in the 'CALC' column:", data['CALC'].unique())
print("Unique values in the 'MTRANS' column:", data['MTRANS'].unique())
print("Unique values in the 'NObeyesdad' column:", data['NObeyesdad'].unique())

row_1292 = data.iloc[1291]  # Note that DataFrame indices are zero-based
print(row_1292)

data.describe()

plt.figure(figsize=(12, 10))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

cat_columns = data.select_dtypes(include='object').columns

num_rows = len(cat_columns)
num_cols = 1

fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(8, 6 * num_rows), sharex=True, gridspec_kw={'hspace': 0.4})

for i, col in enumerate(cat_columns):
    sns.countplot(x=col, data=data, palette='viridis', ax=axes[i])
    axes[i].set_title(f'Count of {col}')

# Show the plots
plt.show()

for col in data.select_dtypes(include=['int64', 'float64']).columns:
    plt.figure(figsize=(8, 6))
    sns.histplot(data[col], kde=True, color='skyblue')
    plt.title(f'Distribution of {col}')
    plt.show()

for col1 in data.select_dtypes(include='object').columns:
    for col2 in data.select_dtypes(include='object').columns:
        if col1 != col2:
            pd.crosstab(data[col1], data[col2]).plot(kind='bar', stacked=True, colormap='viridis')
            plt.title(f'{col1} vs {col2}')
            plt.show()

for col in data.select_dtypes(include=['int64', 'float64']).columns:
    for cat_col in data.select_dtypes(include='object').columns:
        plt.figure(figsize=(10, 6))
        sns.boxplot(x=cat_col, y=col, data=data, palette='viridis')
        plt.title(f'{col} vs {cat_col}')
        plt.show()

"""####Data Preprocessing"""

#mengubah data label menjadi data numerik
#ubah ke string karena umur tidak make sense untuk punya koma
data['Age'] = data['Age'].astype(int)

#ganti string 'Female' jadi int 0 dan string 'Male' jadi int 1
replace_mapping_gender = {'Female' : 0, 'Male' : 1}
data['Gender'] = data['Gender'].replace(replace_mapping_gender)

#ganti string 'no' jadi int 0 dan string 'yes' jadi int 1
replace_mapping = {'no' : 0, 'yes' : 1}
data['family_history_with_overweight'] = data['family_history_with_overweight'].replace(replace_mapping)
data['FAVC'] = data['FAVC'].replace(replace_mapping)
data['SMOKE'] = data['SMOKE'].replace(replace_mapping)
data['SCC'] = data['SCC'].replace(replace_mapping)

#ganti 'no' jadi 0, 'sometimes' jadi 1, 'frequent' jadi 2, 'always' jadi 3
mapping = {'no': 0, 'Sometimes': 1, 'Frequently': 2, 'Always': 3}
data['CAEC'] = data['CAEC'].replace(mapping)
data['CALC'] = data['CALC'].replace(mapping)

#ganti string ke int, transportasi yang paling aktif dikasih nilai rendah,
#transportasi yang paling tidak aktif dikasih nilai tinggi
mtrans_mapping = {'Walking': 0, 'Public_Transportation': 2, 'Automobile': 3, 'Motorbike': 3, 'Bike': 1}
data['MTRANS'] = data['MTRANS'].replace(mtrans_mapping)

#ganti string ke int, yang sehat nilainya rendah, yang paling penyakitan nilainya tinggi
nobeyesdad_mapping = {
    'Insufficient_Weight': 0,
    'Normal_Weight': 1,
    'Overweight_Level_I': 2,
    'Overweight_Level_II': 3,
    'Obesity_Type_I': 4,
    'Obesity_Type_II': 5,
    'Obesity_Type_III': 6
}
data['NObeyesdad'] = data['NObeyesdad'].replace(nobeyesdad_mapping)

plt.figure(figsize=(12, 10))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

#buat kolom 'BMI' atau 'Body Mass Index'
#BMI = berat (kg) / tinggi^2 (m)

data['BMI'] = data['Weight'] / (data['Height'] ** 2)

plt.figure(figsize=(12, 10))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

# Mengubah data null menjadi mean jika ada
print(data.isnull().sum())
data.fillna(data.mean(), inplace=True)

# Feature Scaling
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
numerical_features = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE', 'BMI']
data[numerical_features] = scaler.fit_transform(data[numerical_features])

data

#untuk mendapatkan top 10 feature (Feature Selection, bisa lebih kecil sih, 5 juga boleh)
correlations_with_target = data.corr()['NObeyesdad'].abs()
top_features = correlations_with_target.sort_values(ascending=False).head(10).index
data_subset = data[top_features]

print(correlations_with_target[top_features])

y = data_subset['NObeyesdad']
X = data_subset.drop(columns=['NObeyesdad'])

"""#Ringkasan Metode

####Baseline Model

Ada 3 ensemble yang ada, yaitu Bagging, Boosting, dan Stacking. Disini, kami akan mencoba ketiganya untuk memutuskan yang mana yang dijadikan baseline model
"""

# Import library yang diperlukan
import time
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report
# Split data menjadi training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Mencoba Bagging dengan Random Forrest
start_time = time.time()
rf_model = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42)
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)
rf_accuracy_baseline = accuracy_score(y_test, rf_predictions)
end_time = time.time()
elapsed_time_baseline = end_time - start_time
print(f"Random Forest Accuracy: {rf_accuracy_baseline}")
print(f"Time taken to train and predict: {elapsed_time_baseline} seconds")

#5 fold cross-validation
start_time_cv = time.time()
cross_val_scores_baseline = cross_val_score(rf_model, X_train, y_train, cv=5)
end_time_cv = time.time()
elapsed_time_cv_baseline = end_time_cv - start_time_cv

print("Random Forest Cross-Validation Scores:", cross_val_scores_baseline)
print("Random Forest Mean Cross-Validation Score:", np.mean(cross_val_scores_baseline))
print(f"Time taken for cross-validation: {elapsed_time_cv_baseline} seconds")

#Mencoba Boosting dengan Gradient Boosting
start_time = time.time()
gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_model.fit(X_train, y_train)
gb_predictions = gb_model.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_predictions)
end_time = time.time()
elapsed_time_gb = end_time - start_time

print(f"Gradient Boosting Accuracy: {gb_accuracy}")
print(f"Time taken to train and predict: {elapsed_time_gb} seconds")

#Validasi dengan Cross-Validation
start_time_cv = time.time()
cross_val_scores = cross_val_score(gb_model, X_train, y_train, cv=5)
end_time_cv = time.time()
elapsed_time_cv_gb = end_time_cv - start_time_cv

print("Gradient Boosting Cross-Validation Scores:", cross_val_scores)
print("Gradient Boosting Mean Cross-Validation Score:", np.mean(cross_val_scores))
print(f"Time taken for cross-validation: {elapsed_time_cv_gb} seconds")

gb_model

#Mencoba Stacking
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.naive_bayes import GaussianNB
start_time = time.time()
base_models = [('dt', DecisionTreeClassifier(max_depth=5)),
              ('svm', SVC(gamma=1.0, C=1.0, probability=True)),
              ('gp', GaussianProcessClassifier(RBF(1.0))),
              ('3nn', KNeighborsClassifier(n_neighbors=3)),
              ('rf',RandomForestClassifier(max_depth=3, n_estimators=25)),
              ('gnb', GaussianNB())]

stacking_model = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())
stacking_model.fit(X_train, y_train)
stacking_predictions = stacking_model.predict(X_test)
stacking_accuracy = accuracy_score(y_test, stacking_predictions)
end_time = time.time()
elapsed_time_stacking = end_time - start_time
print(f"Stacking Accuracy: {stacking_accuracy}")
print(f"Time taken to train and predict: {elapsed_time_stacking} seconds")

# kelamaan
#Validasi dengan Cross-Validation
start_time_cv = time.time()
cross_val_scores = cross_val_score(stacking_model, X_train, y_train, cv=5)
end_time_cv = time.time()
elapsed_time_cv_model = end_time_cv - start_time_cv

print("Stacking Cross-Validation Scores:", cross_val_scores)
print("Stacking Mean Cross-Validation Score:", np.mean(cross_val_scores))
print(f"Time taken for cross-validation: {elapsed_time_cv_model} seconds")

stacking_model

"""Dari 3 tipe Ensemble yang ada (Bagging, Boosting, dan Stacking) kami pilih Bagging dengan random forrest untuk menjadi baseline model karena itu memiliki akurasi dan validasi(dengan Cross-Validation) paling tinggi dengan 98,58% dan 98,57%. Boosting tidak dipilih karena akurasinya lebih rendah, dan stacking tidak dipilih karena execution time(untuk mendapatkan akurasi maupun validasi) yang sangat lama.  """

# Confusion Matrix Baseline Model
conf_matrix_model1 = confusion_matrix(y_test, rf_predictions)
print("Confusion Matrix:")
print(conf_matrix_model1)

# Precision, Recall, F1 Score Baseline Model
classification_rep_model1 = classification_report(y_test, rf_predictions)
print("Classification Report:")
print(classification_rep_model1)

"""####Model Exploration

####Model 1
"""

#Model Exploration 1
start_time = time.time()
rf_model1 = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)
rf_model1.fit(X_train, y_train)
rf_predictions = rf_model1.predict(X_test)
rf_accuracy_model1 = accuracy_score(y_test, rf_predictions)
end_time = time.time()
elapsed_time_model1 = end_time - start_time
print(f"Random Forest Accuracy: {rf_accuracy_model1}")
print(f"Time taken to train and predict: {elapsed_time_model1} seconds")

#Validasi dengan Cross-Validation
start_time_cv = time.time()
cross_val_scores_model1 = cross_val_score(rf_model1, X_train, y_train, cv=5)
end_time_cv = time.time()
elapsed_time_cv_model1 = end_time_cv - start_time_cv


print("Random Forest Model Exploration 1 Cross-Validation Scores:", cross_val_scores_model1)
print("Random Forest Model Exploration 1 Mean Cross-Validation Score:", np.mean(cross_val_scores_model1))
print(f"Time taken for cross-validation: {elapsed_time_cv_model1} seconds")

# Confusion Matrix Model 1
conf_matrix_model1 = confusion_matrix(y_test, rf_predictions)
print("Confusion Matrix:")
print(conf_matrix_model1)

# Precision, Recall, F1 Score Model 1
classification_rep_model1 = classification_report(y_test, rf_predictions)
print("Classification Report:")
print(classification_rep_model1)

"""Penjelasan exploration, evaluasi dari exploration

####Model 2
"""

#Model Exploration 2
start_time = time.time()
rf_model2 = RandomForestClassifier(n_estimators=5, max_depth=15, random_state=42)
rf_model2.fit(X_train, y_train)
rf_predictions = rf_model2.predict(X_test)
rf_accuracy_model2 = accuracy_score(y_test, rf_predictions)
end_time = time.time()
elapsed_time_model2 = end_time - start_time
print(f"Random Forest Accuracy: {rf_accuracy_model2}")
print(f"Time taken to train and predict: {elapsed_time_model2} seconds")

#Validasi dengan Cross-Validation
start_time_cv = time.time()
cross_val_scores_model2 = cross_val_score(rf_model2, X_train, y_train, cv=5)
end_time_cv = time.time()
elapsed_time_cv_model2 = end_time_cv - start_time_cv

print("Random Forest Model Exploration 2 Cross-Validation Scores:", cross_val_scores_model2)
print("Random Forest Model Exploration 2 Mean Cross-Validation Score:", np.mean(cross_val_scores_model2))
print(f"Time taken for cross-validation: {elapsed_time_cv_model2} seconds")

# Confusion Matrix Model 2
conf_matrix_model1 = confusion_matrix(y_test, rf_predictions)
print("Confusion Matrix:")
print(conf_matrix_model1)

# Precision, Recall, F1 Score Model 2
classification_rep_model1 = classification_report(y_test, rf_predictions)
print("Classification Report:")
print(classification_rep_model1)

"""Penjelasan exploration, evaluasi dari exploration

####Model 3
"""

#Model Exploration 3
start_time = time.time()
rf_model3 = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
rf_model3.fit(X_train, y_train)
rf_predictions = rf_model3.predict(X_test)
rf_accuracy_model3 = accuracy_score(y_test, rf_predictions)
end_time = time.time()
elapsed_time_model3 = end_time - start_time
print(f"Random Forest Accuracy: {rf_accuracy_model3}")
print(f"Time taken to train and predict: {elapsed_time_model3} seconds")

#Validasi dengan Cross-Validation
start_time_cv = time.time()
cross_val_scores_model3 = cross_val_score(rf_model3, X_train, y_train, cv=5)
end_time_cv = time.time()
elapsed_time_cv_model3 = end_time_cv - start_time_cv

print("Random Forest Model Exploration 3 Cross-Validation Scores:", cross_val_scores_model3)
print("Random Forest Model Exploration 3 Mean Cross-Validation Score:", np.mean(cross_val_scores_model3))
print(f"Time taken for cross-validation: {elapsed_time_cv_model3} seconds")

# Confusion Matrix Model 3
conf_matrix_model1 = confusion_matrix(y_test, rf_predictions)
print("Confusion Matrix:")
print(conf_matrix_model1)

# Precision, Recall, F1 Score Model 3
classification_rep_model1 = classification_report(y_test, rf_predictions)
print("Classification Report:")
print(classification_rep_model1)

"""Penjelasan exploration, evaluasi dari exploration

####Evaluation

Berikut adalah nilai akurasi, validasi, waktu membuat model dan juga waktu memvalidasi dari semua model, dari baseline hingga ketiga model exploration
"""

#Evaluasi hasil baseline model
print(f"Baseline model Random Forest Accuracy: {rf_accuracy_baseline}")
print(f"Time taken to train and predict Baseline Model: {elapsed_time_baseline} seconds")
print("Random Forest Baseline Model Cross-Validation Scores:", cross_val_scores_baseline)
print("Random Forest Baseline Model Mean Cross-Validation Score:", np.mean(cross_val_scores_baseline))
print(f"Time taken for cross-validate Baseline Model: {elapsed_time_cv_baseline} seconds\n")

#Evaluasi hasil model 1
print(f"Model Exploration 1 Random Forest Accuracy: {rf_accuracy_model1}")
print(f"Time taken to train and predict Model Exploration 1: {elapsed_time_model1} seconds")
print("Random Forest Model Exploration 1 Cross-Validation Scores:", cross_val_scores_model1)
print("Random Forest Model Exploration 1 Mean Cross-Validation Score:", np.mean(cross_val_scores_model1))
print(f"Time taken for cross-validate Model Exploration 1: {elapsed_time_cv_model1} seconds\n")

#Evaluasi hasil model 2
print(f"Model Exploration 2 Random Forest Accuracy: {rf_accuracy_model2}")
print(f"Time taken to train and predict Model Exploration 2: {elapsed_time_model2} seconds")
print("Random Forest Model Exploration 2 Cross-Validation Scores:", cross_val_scores_model2)
print("Random Forest Model Exploration 2 Mean Cross-Validation Score:", np.mean(cross_val_scores_model2))
print(f"Time taken for cross-validate Model Exploration 2: {elapsed_time_cv_model2} seconds\n")

#Evaluasi hasil model 3
print(f"Model Exploration 3 Random Forest Accuracy: {rf_accuracy_model3}")
print(f"Time taken to train and predict Model Exploration 3: {elapsed_time_model3} seconds")
print("Random Forest Model Exploration 3 Cross-Validation Scores:", cross_val_scores_model3)
print("Random Forest Model Exploration 3 Mean Cross-Validation Score:", np.mean(cross_val_scores_model3))
print(f"Time taken for cross-validate Model Exploration 3: {elapsed_time_cv_model3} seconds\n")

"""#Hasil dan Analisis"""

models = ['Baseline', 'Exploration 1', 'Exploration 2', 'Exploration 3']

#graph untuk membandingkan nilai akurasi
accuracy_values = np.array([rf_accuracy_baseline, rf_accuracy_model1, rf_accuracy_model2, rf_accuracy_model3])
print(accuracy_values)
print(accuracy_values[1] - accuracy_values[3])

# Plotting the bar chart for accuracy
plt.figure(figsize=(8, 6))
plt.bar(models, accuracy_values, color=['blue', 'orange', 'green', 'red'])
plt.ylim(0.97, 0.99)  # Set y-axis limits for better visualization

# Adding labels and title
plt.xlabel('Random Forest Models')
plt.ylabel('Accuracy')
plt.title('Comparison of Random Forest Models - Accuracy')

# Display the plot
plt.show()

#graph untuk membandingkan nilai cross-validation
cv_mean_baseline = np.mean(cross_val_scores_baseline)
cv_mean_model1 = np.mean(cross_val_scores_model1)
cv_mean_model2 = np.mean(cross_val_scores_model2)
cv_mean_model3 = np.mean(cross_val_scores_model3)
validation_scores = np.array([cv_mean_baseline, cv_mean_model1, cv_mean_model2, cv_mean_model3])

# Plotting the bar chart
plt.bar(models, validation_scores, color=['blue', 'orange', 'green', 'red'])
plt.ylim(0.96, 0.99)  # Set y-axis limits for better visualization

# Adding labels and title
plt.xlabel('Random Forest Models')
plt.ylabel('Mean Cross-Validation Score')
plt.title('Comparison of Cross Validation Values of Random Forest Models')

# Display the plot
plt.show()

#graph untuk membandingkan lama waktu membuat model
elapsed_time_values = np.array([elapsed_time_baseline, elapsed_time_model1, elapsed_time_model2, elapsed_time_model3])
print(elapsed_time_values)
print(elapsed_time_values[1] - elapsed_time_values[3])

plt.figure(figsize=(10, 6))
plt.bar(models, elapsed_time_values, color=['blue', 'orange', 'green', 'red'])
plt.xlabel('Random Forest Models')
plt.ylabel('Time (seconds)')
plt.title('Time Comparison Between Random Forest Models')
plt.show()

#graph untuk membadningkan lama waktu memvalidasi model
elapsed_time_cv_values = np.array([elapsed_time_cv_baseline, elapsed_time_cv_model1, elapsed_time_cv_model2, elapsed_time_cv_model3])

plt.figure(figsize=(10, 6))
plt.bar(models, elapsed_time_cv_values, color=['blue', 'orange', 'green', 'red'])
plt.xlabel('Random Forest Models')
plt.ylabel('Time (seconds)')
plt.title('Validation Time Comparison Between Random Forest Models')
plt.show()

"""Dapat dilihat bahwa model-model yang kami buat mendapatkan nilai akurasi dan mean cross validation yang sangat baik, semuanya mendapatkan nilai akurasi diatas 97%.

Model baseline dengan jumlah tree sebanyak 100 dan kedalaman treenya sebanyak 15 mendapatkan nilai akurasi dan waktu runtime yang menengahi model lainnya, kecuali untuk nilai validasi (nilai validasi baseline paling bagus). Model Eksplorasi 1 dengan jumlah tree sebanyak 200 (2x baseline) dan kedalaman tree yang sama dengan baseline mendapatkan nilai akurasi yang paling baik, namun karena jumlah treenya, mendapatkan waktu runtime yang paling lama. Model Eksplorasi 2 dengan jumlah tree hanya sebanyak 5 (1/20x baseline) dan kedalaman tree yang sama dengan baseline mendapatkan nilai akurasi paling buruk, namun karena jumlah treenya, mendapatkan waktu runtime yang paling cepat. Model Eksplorasi 3 memiliki jumlah tree yang sama dengan Model baseline, namun karena kedalaman treenya hanya 5 menjadi sedikit terhambat dalam nilai akurasi dan validasinya.

Namun perlu dicatat bahwa perbedaan nilai akurasi dari model yang paling akurat dengan model paling tidak akurat hanyalah 1,4% saja, sedangkan perbedaan runtime diantara kedua model tersebut hanya 0.172 detik.

#Kesimpulan

Dari perbandingan keempat model Random Forest, empat-empatnya menggunakan metode ensemble Bagging dengan variasi jumlah pohon, dan kedalaman pohon. Model-model tersebut digunakan untuk membandingkan akurasi dan performa relatif terhadap baseline model.

Semua model mendapatkan nilai akurasi yang sangat baik, semuanya mendapatkan nilai akurasi diatas 97,3%, nilai validasi diatas 96,7%. Perbedaan Waktu antar model sangat kecil, oleh karena itu dapat diabaikan. Sehingga, dapat disimpulkan lebih baik menggunakan model dengan akurasi dan validasi yang paling tinggi yaitu Model Baseline dengan akurasi 98,58 % dan validasi 98,57%.
"""